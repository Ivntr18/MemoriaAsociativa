{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.load('T22Audio/feat_X.npy')\n",
    "Y=np.load('T22Audio/feat_Y.npy')\n",
    "Zeros=X==0\n",
    "offset=X.min()\n",
    "X=X-X.min()\n",
    "X[Zeros]=0\n",
    "X=X.reshape(X.shape[0],13,13)\n",
    "dicY={'a':0,'b':1,'d':2,'e':3,'f':4,'g':5,'i':6,'k':7,'l':8,'m':9,'n':10,'n~':11,'o':12,'p':13,'r':14,'r(':15,'s':16,'t':17,'tS':18,'u':19,'x':20,'Z':21}\n",
    "Y=[dicY[key] for key in Y]\n",
    "Y=np.array(Y, dtype='uint8')\n",
    "sizeInp=X.shape[0]\n",
    "permutation = np.random.permutation(sizeInp)\n",
    "X=X[permutation]\n",
    "Y=Y[permutation]\n",
    "train_X=X[:int(0.8*sizeInp)]\n",
    "test_X=X[int(0.8*sizeInp):]\n",
    "train_Y=Y[:int(0.8*sizeInp)]\n",
    "test_Y=Y[int(0.8*sizeInp):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train_X/X.max()\n",
    "test_X=test_X/X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(13, 13)),\n",
    "    keras.layers.Dense(220, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(440, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(22, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=keras.optimizers.SGD(lr=0.05, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(optimizer=sgd, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 184913 samples, validate on 46229 samples\n",
      "Epoch 1/100\n",
      "184913/184913 [==============================] - 8s 43us/step - loss: 1.2898 - acc: 0.5777 - val_loss: 1.0592 - val_acc: 0.6469\n",
      "Epoch 2/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 1.0006 - acc: 0.6619 - val_loss: 0.9442 - val_acc: 0.6796\n",
      "Epoch 3/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.9137 - acc: 0.6904 - val_loss: 0.9575 - val_acc: 0.6719\n",
      "Epoch 4/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.8613 - acc: 0.7073 - val_loss: 0.8531 - val_acc: 0.7105\n",
      "Epoch 5/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.8242 - acc: 0.7203 - val_loss: 0.8282 - val_acc: 0.7168\n",
      "Epoch 6/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.7970 - acc: 0.7296 - val_loss: 0.8996 - val_acc: 0.6918\n",
      "Epoch 7/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.7742 - acc: 0.7362 - val_loss: 0.8982 - val_acc: 0.6884\n",
      "Epoch 8/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.7552 - acc: 0.7429 - val_loss: 0.7443 - val_acc: 0.7472\n",
      "Epoch 9/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.7384 - acc: 0.7476 - val_loss: 0.7490 - val_acc: 0.7405\n",
      "Epoch 10/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.7235 - acc: 0.7536 - val_loss: 0.7412 - val_acc: 0.7466\n",
      "Epoch 11/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.7109 - acc: 0.7575 - val_loss: 0.7069 - val_acc: 0.7581\n",
      "Epoch 12/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6979 - acc: 0.7613 - val_loss: 0.8157 - val_acc: 0.7258\n",
      "Epoch 13/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6878 - acc: 0.7641 - val_loss: 0.6868 - val_acc: 0.7657\n",
      "Epoch 14/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6765 - acc: 0.7685 - val_loss: 0.7044 - val_acc: 0.7586\n",
      "Epoch 15/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6679 - acc: 0.7716 - val_loss: 0.6757 - val_acc: 0.7695\n",
      "Epoch 16/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6579 - acc: 0.7751 - val_loss: 0.6733 - val_acc: 0.7690\n",
      "Epoch 17/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6497 - acc: 0.7778 - val_loss: 0.6832 - val_acc: 0.7646\n",
      "Epoch 18/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6419 - acc: 0.7804 - val_loss: 0.6780 - val_acc: 0.7668\n",
      "Epoch 19/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.6341 - acc: 0.7826 - val_loss: 0.6655 - val_acc: 0.7710\n",
      "Epoch 20/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.6278 - acc: 0.7854 - val_loss: 0.6879 - val_acc: 0.7655\n",
      "Epoch 21/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.6213 - acc: 0.7868 - val_loss: 0.6575 - val_acc: 0.7779\n",
      "Epoch 22/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6154 - acc: 0.7892 - val_loss: 0.6332 - val_acc: 0.7854\n",
      "Epoch 23/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6085 - acc: 0.7908 - val_loss: 0.6608 - val_acc: 0.7736\n",
      "Epoch 24/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.6029 - acc: 0.7936 - val_loss: 0.6544 - val_acc: 0.7767\n",
      "Epoch 25/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5973 - acc: 0.7949 - val_loss: 0.6425 - val_acc: 0.7838\n",
      "Epoch 26/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5923 - acc: 0.7961 - val_loss: 0.6740 - val_acc: 0.7707\n",
      "Epoch 27/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5882 - acc: 0.7977 - val_loss: 0.6349 - val_acc: 0.7827\n",
      "Epoch 28/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5829 - acc: 0.8003 - val_loss: 0.6202 - val_acc: 0.7906\n",
      "Epoch 29/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.5774 - acc: 0.8014 - val_loss: 0.6163 - val_acc: 0.7908\n",
      "Epoch 30/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5726 - acc: 0.8034 - val_loss: 0.6185 - val_acc: 0.7902\n",
      "Epoch 31/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5687 - acc: 0.8042 - val_loss: 0.6257 - val_acc: 0.7873\n",
      "Epoch 32/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5645 - acc: 0.8056 - val_loss: 0.6314 - val_acc: 0.7869\n",
      "Epoch 33/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5604 - acc: 0.8068 - val_loss: 0.6499 - val_acc: 0.7782\n",
      "Epoch 34/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5557 - acc: 0.8090 - val_loss: 0.6280 - val_acc: 0.7869\n",
      "Epoch 35/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5532 - acc: 0.8095 - val_loss: 0.6101 - val_acc: 0.7936\n",
      "Epoch 36/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.5472 - acc: 0.8112 - val_loss: 0.6113 - val_acc: 0.7942\n",
      "Epoch 37/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.5448 - acc: 0.8132 - val_loss: 0.6630 - val_acc: 0.7746\n",
      "Epoch 38/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5410 - acc: 0.8132 - val_loss: 0.6059 - val_acc: 0.7945\n",
      "Epoch 39/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5376 - acc: 0.8141 - val_loss: 0.6143 - val_acc: 0.7934\n",
      "Epoch 40/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5346 - acc: 0.8152 - val_loss: 0.6114 - val_acc: 0.7931\n",
      "Epoch 41/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5296 - acc: 0.8168 - val_loss: 0.6000 - val_acc: 0.7979\n",
      "Epoch 42/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5269 - acc: 0.8181 - val_loss: 0.5947 - val_acc: 0.7987\n",
      "Epoch 43/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5236 - acc: 0.8186 - val_loss: 0.5990 - val_acc: 0.7973\n",
      "Epoch 44/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5200 - acc: 0.8211 - val_loss: 0.6320 - val_acc: 0.7872\n",
      "Epoch 45/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5171 - acc: 0.8215 - val_loss: 0.6027 - val_acc: 0.7944\n",
      "Epoch 46/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5136 - acc: 0.8227 - val_loss: 0.6393 - val_acc: 0.7884\n",
      "Epoch 47/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5114 - acc: 0.8228 - val_loss: 0.5962 - val_acc: 0.7971\n",
      "Epoch 48/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5088 - acc: 0.8233 - val_loss: 0.5968 - val_acc: 0.7985\n",
      "Epoch 49/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.5054 - acc: 0.8257 - val_loss: 0.5989 - val_acc: 0.7991\n",
      "Epoch 50/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.5017 - acc: 0.8261 - val_loss: 0.5938 - val_acc: 0.8011\n",
      "Epoch 51/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4987 - acc: 0.8269 - val_loss: 0.5928 - val_acc: 0.7995\n",
      "Epoch 52/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4968 - acc: 0.8275 - val_loss: 0.5931 - val_acc: 0.8003\n",
      "Epoch 53/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4936 - acc: 0.8291 - val_loss: 0.5838 - val_acc: 0.8046\n",
      "Epoch 54/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4909 - acc: 0.8307 - val_loss: 0.5991 - val_acc: 0.8008\n",
      "Epoch 55/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4883 - acc: 0.8306 - val_loss: 0.6028 - val_acc: 0.7981\n",
      "Epoch 56/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4857 - acc: 0.8309 - val_loss: 0.5948 - val_acc: 0.7997\n",
      "Epoch 57/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.4828 - acc: 0.8329 - val_loss: 0.6208 - val_acc: 0.7884\n",
      "Epoch 58/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4809 - acc: 0.8322 - val_loss: 0.5927 - val_acc: 0.8029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4773 - acc: 0.8337 - val_loss: 0.6137 - val_acc: 0.7963\n",
      "Epoch 60/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4757 - acc: 0.8350 - val_loss: 0.6019 - val_acc: 0.8003\n",
      "Epoch 61/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4720 - acc: 0.8359 - val_loss: 0.5678 - val_acc: 0.8103\n",
      "Epoch 62/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4700 - acc: 0.8358 - val_loss: 0.6333 - val_acc: 0.7857\n",
      "Epoch 63/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4687 - acc: 0.8369 - val_loss: 0.5872 - val_acc: 0.8024\n",
      "Epoch 64/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4655 - acc: 0.8379 - val_loss: 0.5986 - val_acc: 0.8001\n",
      "Epoch 65/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4633 - acc: 0.8386 - val_loss: 0.5915 - val_acc: 0.8030\n",
      "Epoch 66/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4602 - acc: 0.8398 - val_loss: 0.5907 - val_acc: 0.8044\n",
      "Epoch 67/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4594 - acc: 0.8398 - val_loss: 0.6230 - val_acc: 0.7962\n",
      "Epoch 68/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4556 - acc: 0.8405 - val_loss: 0.5944 - val_acc: 0.8023\n",
      "Epoch 69/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.4542 - acc: 0.8414 - val_loss: 0.5971 - val_acc: 0.8020\n",
      "Epoch 70/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.4517 - acc: 0.8421 - val_loss: 0.6016 - val_acc: 0.8007\n",
      "Epoch 71/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4498 - acc: 0.8426 - val_loss: 0.6102 - val_acc: 0.8005\n",
      "Epoch 72/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.4474 - acc: 0.8445 - val_loss: 0.5968 - val_acc: 0.8034\n",
      "Epoch 73/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4445 - acc: 0.8448 - val_loss: 0.5875 - val_acc: 0.8066\n",
      "Epoch 74/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4428 - acc: 0.8452 - val_loss: 0.5897 - val_acc: 0.8042\n",
      "Epoch 75/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4401 - acc: 0.8458 - val_loss: 0.5928 - val_acc: 0.8063\n",
      "Epoch 76/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4389 - acc: 0.8465 - val_loss: 0.6314 - val_acc: 0.7940\n",
      "Epoch 77/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4375 - acc: 0.8474 - val_loss: 0.6002 - val_acc: 0.8038\n",
      "Epoch 78/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4349 - acc: 0.8470 - val_loss: 0.5857 - val_acc: 0.8063\n",
      "Epoch 79/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4325 - acc: 0.8481 - val_loss: 0.5943 - val_acc: 0.8059\n",
      "Epoch 80/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.4303 - acc: 0.8492 - val_loss: 0.5962 - val_acc: 0.8027\n",
      "Epoch 81/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4281 - acc: 0.8493 - val_loss: 0.6439 - val_acc: 0.7925\n",
      "Epoch 82/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4272 - acc: 0.8504 - val_loss: 0.6127 - val_acc: 0.7989\n",
      "Epoch 83/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4238 - acc: 0.8507 - val_loss: 0.6101 - val_acc: 0.8016\n",
      "Epoch 84/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4224 - acc: 0.8518 - val_loss: 0.6084 - val_acc: 0.8022\n",
      "Epoch 85/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4206 - acc: 0.8525 - val_loss: 0.6038 - val_acc: 0.8056\n",
      "Epoch 86/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4174 - acc: 0.8532 - val_loss: 0.5894 - val_acc: 0.8059\n",
      "Epoch 87/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4169 - acc: 0.8536 - val_loss: 0.6194 - val_acc: 0.7969\n",
      "Epoch 88/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4145 - acc: 0.8535 - val_loss: 0.6143 - val_acc: 0.8020\n",
      "Epoch 89/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4125 - acc: 0.8548 - val_loss: 0.6092 - val_acc: 0.8019\n",
      "Epoch 90/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4101 - acc: 0.8559 - val_loss: 0.6202 - val_acc: 0.7991\n",
      "Epoch 91/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.4089 - acc: 0.8562 - val_loss: 0.6045 - val_acc: 0.8039\n",
      "Epoch 92/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4060 - acc: 0.8565 - val_loss: 0.6073 - val_acc: 0.8036\n",
      "Epoch 93/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4049 - acc: 0.8565 - val_loss: 0.6385 - val_acc: 0.7993\n",
      "Epoch 94/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4037 - acc: 0.8579 - val_loss: 0.5945 - val_acc: 0.8073\n",
      "Epoch 95/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4023 - acc: 0.8573 - val_loss: 0.6075 - val_acc: 0.8024\n",
      "Epoch 96/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.4004 - acc: 0.8578 - val_loss: 0.6052 - val_acc: 0.8061\n",
      "Epoch 97/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.3985 - acc: 0.8586 - val_loss: 0.6474 - val_acc: 0.7917\n",
      "Epoch 98/100\n",
      "184913/184913 [==============================] - 7s 36us/step - loss: 0.3971 - acc: 0.8600 - val_loss: 0.6111 - val_acc: 0.8070\n",
      "Epoch 99/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.3959 - acc: 0.8602 - val_loss: 0.6442 - val_acc: 0.7909\n",
      "Epoch 100/100\n",
      "184913/184913 [==============================] - 7s 37us/step - loss: 0.3937 - acc: 0.8605 - val_loss: 0.6075 - val_acc: 0.8089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f885fb04668>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=100, validation_data=(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for layer in model.layers:\n",
    "   w = layer.get_weights()\n",
    "   all_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.4929049e-02,  5.3048217e-01, -4.5747450e-03,  6.0277241e-01,\n",
       "        7.7750385e-02, -1.1600584e+00, -5.3903723e-01, -1.6906472e+00,\n",
       "        3.3169991e-01, -9.6787417e-01, -4.2177236e-01,  2.1818824e-01,\n",
       "        9.1007060e-01, -1.1213694e+00,  9.7857360e-03, -5.9380543e-01,\n",
       "       -2.9633674e-04,  3.0850330e-01, -1.3269011e+00, -4.1660942e-02,\n",
       "        0.0000000e+00,  0.0000000e+00,  7.0806789e-01,  8.6511475e-01,\n",
       "       -2.5072264e-02,  3.6205772e-02, -1.7848340e-03,  4.0982097e-01,\n",
       "       -6.5771639e-03, -2.5260197e-02, -2.1304950e-01, -3.7987834e-01,\n",
       "       -8.8366824e-01, -8.8097423e-01,  1.0680401e+00,  1.5043755e+00,\n",
       "       -3.5329816e-01, -3.0801874e-01,  7.8158705e-03,  0.0000000e+00,\n",
       "        0.0000000e+00, -1.0878881e+00,  2.0817424e-01, -1.0134062e+00,\n",
       "       -1.6028447e-02,  6.3150381e-03,  5.2016205e-01, -7.0770504e-03,\n",
       "       -5.1567656e-01,  4.0037337e-01, -7.2874492e-01, -6.5662098e-01,\n",
       "       -1.1072156e+00, -3.5748474e-02, -2.6720688e-01,  3.9069004e-02,\n",
       "       -4.9468083e+00, -7.2028220e-01, -2.6490722e+00,  0.0000000e+00,\n",
       "       -4.9223754e-01, -7.3561678e-03,  4.0264060e-03,  2.6107195e-01,\n",
       "       -1.9056413e-03, -1.0624542e-02,  1.1161400e-01, -6.7201680e-01,\n",
       "        3.0535232e-02, -4.3992969e-01, -1.2370527e+00, -7.9112387e-01,\n",
       "       -4.2946185e-03,  4.2969971e+00, -6.9717795e-02,  0.0000000e+00,\n",
       "       -3.1951943e-01, -3.0950838e-01,  0.0000000e+00, -5.4503983e-01,\n",
       "       -4.0933397e-01, -1.0826546e+00, -2.4061170e+00,  0.0000000e+00,\n",
       "       -2.8402848e+00,  5.5269325e-01, -4.9551263e-01,  0.0000000e+00,\n",
       "        2.7074102e-01, -3.0843374e-01,  1.0065765e+00,  2.4792140e+00,\n",
       "       -3.9313322e-01, -1.2283583e-02,  5.5595094e-01,  1.5282171e+00,\n",
       "       -1.3208573e+00,  6.3110489e-01, -2.9773779e-02, -8.1522763e-03,\n",
       "        1.6186677e-01, -2.4987793e-01, -7.4941075e-01, -4.0977603e-01,\n",
       "       -2.5859656e-02, -6.7492551e-01, -6.7032343e-03, -3.2584434e+00,\n",
       "        1.6988862e-02, -2.9994684e-01, -1.1482108e-03, -3.5387629e-01,\n",
       "       -3.3058349e-02, -2.6895735e-01, -2.4732278e-01,  2.8683114e+00,\n",
       "       -8.9823407e-01, -1.9120326e-02, -8.4473711e-01,  3.7180817e-01,\n",
       "        1.0838650e+00, -5.0824231e-01,  3.9411843e-01,  5.3125095e-01,\n",
       "        2.5051928e+00, -1.8524930e-01,  3.1382212e-01, -1.2654874e-01,\n",
       "       -1.6226724e-01,  7.6297021e-01, -3.0457836e-01,  0.0000000e+00,\n",
       "       -1.5142865e+00, -1.6695859e+00,  9.0495324e-01, -2.6065190e+00,\n",
       "       -6.1453190e-03,  8.3517647e-01,  3.8738608e-01,  6.5507227e-01,\n",
       "        5.0790277e-03,  4.9951619e-01,  8.1325114e-01,  8.0231577e-01,\n",
       "       -6.3222766e-01,  6.2394816e-01,  0.0000000e+00, -5.2988374e-01,\n",
       "        1.2414826e+00, -7.2880715e-01, -1.4745333e+00, -3.3692159e-03,\n",
       "        2.6490685e-01,  1.5603496e+00,  1.3680345e-01, -6.0999382e-01,\n",
       "        6.6609013e-01, -1.0843766e+00, -2.0423820e+00,  0.0000000e+00,\n",
       "        0.0000000e+00, -9.1083042e-02,  4.2706419e-02, -9.2115521e-01,\n",
       "       -9.3724382e-01, -5.3911430e-01,  0.0000000e+00,  8.5591400e-01,\n",
       "       -9.2134532e-03, -6.5182284e-03, -1.1360956e-02, -7.2804475e-01,\n",
       "       -6.9876313e-02,  2.3058368e-02,  0.0000000e+00,  1.5716102e-02,\n",
       "        4.3741289e-01, -6.4505398e-01,  1.9310583e+00,  1.1860733e+00,\n",
       "        1.8284351e-01, -1.3066525e+00,  1.0466420e-01,  0.0000000e+00,\n",
       "       -2.9505542e-01, -9.5573050e-01, -1.1533215e+00, -4.4688645e-01,\n",
       "        1.4199925e-02, -1.1487341e+00, -1.1830727e+00, -1.3165204e-01,\n",
       "       -2.5748778e-02,  7.1435928e-04, -8.3095044e-01, -2.1583037e+00,\n",
       "        0.0000000e+00, -7.4538589e-01,  5.0991207e-01,  0.0000000e+00,\n",
       "       -2.8457236e-01, -1.8375188e+00,  0.0000000e+00,  3.1554630e-01,\n",
       "        2.3564937e-02, -1.4140092e-01,  2.3494123e-01, -1.4836860e+00,\n",
       "       -1.0021211e+00, -2.8307793e-01,  8.9266658e-01,  9.8340589e-01,\n",
       "        2.2003441e+00, -8.8478908e-02, -2.2782114e-01,  1.0200922e+00,\n",
       "        0.0000000e+00,  2.9668145e-02, -4.2060301e-02,  1.4597757e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.29710168e-01, -3.31659734e-01, -4.17743623e-01, -2.18016788e-01,\n",
       "       -9.11898136e-01, -1.52209446e-01, -6.60444498e-01, -6.44696414e-01,\n",
       "       -8.23727190e-01, -1.71148986e-01,  4.10671234e-02, -3.05495948e-01,\n",
       "        3.03565990e-02,  6.92630351e-01, -4.94317949e-01,  2.82953102e-02,\n",
       "       -5.45569602e-03,  1.55914472e-02, -3.94847155e-01, -6.40891671e-01,\n",
       "        2.48488244e-02, -6.32955670e-01,  1.68954045e-01, -3.40410888e-01,\n",
       "       -3.19063902e-01, -2.22032771e-01, -4.28591669e-01, -3.51604447e-03,\n",
       "       -2.62921024e-02, -5.82091957e-02,  2.84674197e-01, -8.17230642e-02,\n",
       "        4.78835136e-01, -5.33397794e-01, -4.25134510e-01,  4.14287299e-01,\n",
       "        5.62653780e-01,  3.59982252e-01,  2.23973412e-02, -1.19621962e-01,\n",
       "        8.46407004e-03,  4.22335446e-01, -8.66457727e-03, -2.57433616e-02,\n",
       "       -2.68913388e-01, -4.33174282e-01, -4.14096206e-01, -5.36361001e-02,\n",
       "       -1.83229834e-01, -9.43126902e-02, -9.56229508e-01, -4.45703745e-01,\n",
       "       -3.54730695e-01,  3.31286132e-01,  3.22356045e-01, -1.10545769e-01,\n",
       "        1.75699070e-01, -3.66157442e-02, -5.87010324e-01, -5.09144962e-02,\n",
       "        1.88891590e-01,  1.05208773e-02,  2.31891945e-01, -3.43259037e-01,\n",
       "        3.06552291e-01,  4.50634927e-01, -1.54537976e-01, -1.26363650e-01,\n",
       "       -4.06923205e-01, -8.55129883e-02, -4.61436361e-02,  1.67826101e-01,\n",
       "       -1.87943757e-01,  3.61120135e-01, -1.27464950e-01,  2.18286570e-02,\n",
       "       -1.83504865e-01, -4.45147038e-01,  1.67836562e-01, -1.94434047e-01,\n",
       "        9.59464237e-02, -2.91653961e-01, -5.67450404e-01, -4.47569460e-01,\n",
       "        4.38466817e-02,  8.10830072e-02, -5.75109720e-01, -5.80690242e-03,\n",
       "       -5.65439202e-02,  9.35822651e-02,  2.44199708e-01, -3.87442559e-01,\n",
       "       -7.75161907e-02,  4.91578311e-01, -8.11205246e-03, -3.03119123e-01,\n",
       "       -1.81746230e-01, -3.41884583e-01, -2.13864982e-01, -2.56399304e-01,\n",
       "       -8.55297521e-02, -3.10686946e-01,  4.38536331e-02, -9.97717604e-02,\n",
       "       -1.37745693e-01, -8.03733706e-01, -5.90336695e-02, -5.58225214e-02,\n",
       "        3.75890695e-02,  4.39602107e-01, -3.64491791e-01,  8.95530800e-04,\n",
       "       -2.08079964e-01,  1.38342917e-01, -2.68833995e-01, -7.53912508e-01,\n",
       "       -3.68461907e-01, -1.09806344e-01, -6.88790008e-02, -9.71627794e-03,\n",
       "       -6.90852880e-01,  4.95166898e-01, -5.21889739e-02, -2.15104744e-01,\n",
       "        2.36925930e-01, -1.29185975e-01,  1.00514807e-01, -2.14139223e-02,\n",
       "       -2.15673000e-01, -6.00349754e-02, -6.59832299e-01, -7.20224082e-02,\n",
       "        5.73380709e-01,  7.16510952e-01,  2.83045564e-02,  4.33142811e-01,\n",
       "       -2.13186130e-01,  5.28212726e-01,  3.20253313e-01,  1.02256564e-02,\n",
       "       -3.19265991e-01,  2.54246980e-01, -9.03968737e-02, -5.75280786e-01,\n",
       "       -4.31059301e-01, -2.04039410e-01,  2.01159418e-01, -2.66713202e-01,\n",
       "        1.77605171e-02, -1.34206578e-01, -8.22093964e-01,  6.48362756e-01,\n",
       "        6.80819988e-01, -5.58304250e-01,  2.22001657e-01, -7.01719001e-02,\n",
       "       -3.00696939e-01, -2.68043995e-01,  6.48422241e-01,  4.80282426e-01,\n",
       "       -6.32885456e-01,  3.68076324e-01, -3.73397112e-01, -5.57184458e-01,\n",
       "       -1.61353067e-01, -5.41531965e-02, -6.40446171e-02, -9.21775937e-01,\n",
       "       -1.22869980e+00, -2.35595018e-01,  5.82212627e-01,  4.68197167e-01,\n",
       "        2.86359638e-01, -3.32130671e-01, -2.94609129e-01,  2.47812539e-01,\n",
       "        3.48468125e-02, -1.12190291e-01,  1.85123146e-01,  9.94965360e-02,\n",
       "       -3.02714594e-02,  3.38193253e-02, -1.13227546e-01,  4.98044789e-01,\n",
       "       -1.75907258e-02, -3.74248862e-01, -2.04549447e-01, -5.63789010e-01,\n",
       "        5.03973067e-01, -2.06286177e-01, -3.56115922e-02, -2.54376382e-01,\n",
       "        7.09909976e-01,  1.24926090e-01, -2.74411201e-01,  6.78338647e-01,\n",
       "       -1.31408334e-01, -1.69695452e-01,  1.78725004e-01, -4.47647929e-01,\n",
       "       -4.17460948e-01,  1.39424026e-01,  9.46331978e-01, -2.17230588e-01,\n",
       "       -3.66378784e-01,  8.76493473e-03, -2.35602304e-01,  2.15677455e-01,\n",
       "       -1.77490994e-01, -1.57941088e-01,  3.68058503e-01, -1.20867193e-01,\n",
       "        8.53693306e-01,  1.19499959e-01,  8.56679752e-02, -1.73423886e-01,\n",
       "       -3.66145760e-01, -3.51049185e-01,  8.01843584e-01,  1.90654889e-01,\n",
       "       -2.76666999e-01, -2.96839550e-02, -2.54446447e-01, -3.21713626e-01,\n",
       "        3.54017079e-01, -2.14344963e-01,  3.27416897e-01,  4.61682260e-01,\n",
       "       -2.76145816e-01, -4.99315858e-01,  2.62247652e-01,  3.76380607e-02,\n",
       "        1.72287941e-01,  7.18398690e-02,  1.89317223e-02,  5.88323236e-01,\n",
       "       -2.30214030e-01,  5.90506315e-01, -8.75187516e-02,  3.27086709e-02,\n",
       "       -1.88461736e-01, -3.05114001e-01, -3.23721766e-01,  3.90160680e-01,\n",
       "        7.08569437e-02,  1.89795047e-01, -1.36387214e-01,  1.86934397e-01,\n",
       "       -3.42161618e-02,  2.49744609e-01,  7.55155206e-01,  3.31225514e-01,\n",
       "        5.13963223e-01,  3.67667042e-02,  4.05493006e-02, -2.59037673e-01,\n",
       "       -5.22463441e-01,  2.13680774e-01, -3.72397155e-01, -3.51300031e-01,\n",
       "        1.24813788e-01, -2.33755365e-01,  4.41625684e-01,  5.59391454e-02,\n",
       "        5.78825593e-01, -1.32181689e-01,  2.04411015e-01,  3.61732423e-01,\n",
       "        1.50207683e-01, -6.70505583e-01,  2.62673229e-01,  4.44723014e-03,\n",
       "       -3.74192238e-01,  1.74413860e-01, -5.22026837e-01, -9.83506963e-02,\n",
       "        1.78224668e-02,  1.72980413e-01,  3.36735509e-02,  5.95932543e-01,\n",
       "        1.56360149e-01,  1.35088101e-01, -5.51086187e-01, -2.27691323e-01,\n",
       "       -4.75176185e-01, -5.44422626e-01, -4.30166155e-01, -2.31187090e-01,\n",
       "       -5.11451028e-02, -2.25317091e-01, -2.13634402e-01,  1.42396942e-01,\n",
       "        2.76925594e-01, -1.80737913e-01,  8.19749311e-02,  8.14120919e-02,\n",
       "       -5.51918969e-02, -2.37628415e-01,  7.64615089e-02,  2.47958899e-01,\n",
       "       -5.43229997e-01,  1.09078079e-01, -5.43710709e-01, -1.95220277e-01,\n",
       "       -3.50744724e-01,  8.15253779e-02,  1.74371526e-01, -3.44805449e-01,\n",
       "       -3.95033777e-01,  3.30799133e-01,  3.41495015e-02,  3.36849391e-01,\n",
       "        2.90008396e-01, -2.38626122e-01,  3.88454795e-01, -2.94322595e-02,\n",
       "       -2.38028206e-02,  2.39729747e-01,  8.23524818e-02, -1.48552760e-01,\n",
       "       -2.63750941e-01,  6.12149537e-01,  1.64460689e-01, -5.39682567e-01,\n",
       "       -4.25823152e-01, -9.11685079e-02, -3.11998814e-01, -5.38600504e-01,\n",
       "        5.13158441e-01,  3.33494991e-01, -1.79517735e-02,  9.65780839e-02,\n",
       "       -2.30597228e-01, -4.26459491e-01, -1.11801460e-01, -1.71010047e-01,\n",
       "       -1.40878735e-02, -2.67666638e-01,  3.48399699e-01, -6.08052313e-02,\n",
       "       -2.02596039e-01,  2.16147646e-01, -4.37246531e-01,  5.47423601e-01,\n",
       "       -4.97286677e-01, -4.14484233e-01, -2.77762622e-01, -3.27159092e-02,\n",
       "       -5.33535838e-01,  1.02436012e-02,  6.76304325e-02, -6.52557969e-01,\n",
       "       -2.44643360e-01, -2.67438293e-01, -8.23438615e-02,  1.41204506e-01,\n",
       "       -4.19081032e-01,  8.02856535e-02, -7.44804740e-03, -2.79802978e-01,\n",
       "       -4.52178746e-01,  2.53912926e-01,  2.42625684e-01, -6.14879653e-02,\n",
       "       -1.03615455e-01, -9.80564654e-02, -7.02689886e-01,  2.88493535e-03,\n",
       "       -6.01001596e-03, -1.67509913e-01,  9.59289744e-02, -5.52706897e-01,\n",
       "       -3.91370595e-01, -2.90213525e-01,  8.12021792e-02, -1.50867447e-01,\n",
       "       -1.84443086e-01, -2.99305648e-01,  5.03662288e-01, -3.54592413e-01,\n",
       "       -2.89255064e-02, -1.62620470e-01, -3.12725186e-01, -1.59320608e-01,\n",
       "        2.70256072e-01, -5.74728996e-02, -4.18872386e-01,  2.36202464e-01,\n",
       "       -6.04968131e-01, -1.38441354e-01, -1.01128139e-01,  6.17863297e-01,\n",
       "       -8.11894417e-01,  4.66809720e-02,  2.18292370e-01, -1.23397164e-01,\n",
       "       -5.94098449e-01, -1.86283171e-01, -6.55326962e-01, -3.79593521e-01,\n",
       "        1.49426401e-01, -3.74334514e-01,  2.34668944e-02, -1.80073172e-01,\n",
       "       -5.83887219e-01, -5.93351483e-01, -5.22009671e-01, -2.19800007e-02,\n",
       "       -2.89078295e-01, -1.77646987e-02, -8.63305867e-01,  1.01406068e-01,\n",
       "       -2.18236670e-01,  4.75868404e-01,  4.06375043e-02, -4.30105209e-01,\n",
       "        2.28986800e-01, -8.83666933e-01,  2.30752349e-01,  2.24295124e-01,\n",
       "        1.13488579e+00,  1.86727747e-01,  4.84628715e-02, -1.84173450e-01,\n",
       "        1.19709015e-01, -2.61720866e-01, -6.27721786e-01, -4.88793403e-01,\n",
       "        4.40661103e-01, -1.83751807e-01, -1.89855471e-01, -4.14886802e-01,\n",
       "        3.89599800e-02, -1.45716205e-01,  3.64950486e-02, -1.37322351e-01,\n",
       "       -2.78103501e-01, -7.16192648e-02,  1.44770533e-01, -6.23669587e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42695224, -0.19081813,  0.11621566, ..., -0.33536398,\n",
       "        -0.05244092,  0.02520592],\n",
       "       [ 0.30221143,  0.2226755 ,  0.4556592 , ...,  0.28280792,\n",
       "        -0.0959378 , -0.35765564],\n",
       "       [-0.09252478,  0.65858835,  0.21123666, ..., -0.384522  ,\n",
       "         0.0810954 ,  0.58949894],\n",
       "       ...,\n",
       "       [ 0.0750394 , -0.19279373,  0.17000426, ..., -0.00679524,\n",
       "         0.18455437,  0.36261737],\n",
       "       [-0.1356139 ,  0.04931086,  0.47636428, ...,  0.02002008,\n",
       "         0.31689736, -0.10657631],\n",
       "       [ 0.34062478,  0.0347233 ,  0.08123791, ..., -0.8730093 ,\n",
       "         0.34356192,  0.13432024]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
